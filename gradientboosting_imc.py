# -*- coding: utf-8 -*-
"""GradientBoosting_IMC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1noizpml-e_rrIrGgZleM9Pd3eJNy-SUU
"""

# 1. importações
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB

# 2. carregar o dataset
df = pd.read_csv('/content/Obesity.csv')


# 2.1 Criar coluna BMI (IMC)
df['BMI'] = df['Weight'] / (df['Height'] ** 2)

print(df.head())

print(df.info())

print(df.describe())

print(df.columns)

df['Obesity'].unique()

np.unique(df['Obesity'], return_counts=True)

# separar features e target
X = df.drop('Obesity', axis=1)
y = df['Obesity']

# identificar colunas categóricas e numéricas
num_features = ['Age', 'Height', 'Weight', 'BMI']

cat_features = [col for col in X.columns if col not in num_features]


# pré-processamento
preprocess = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),
    ('num', 'passthrough', num_cols)
])

# split fixo
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Modelo 1: Gradient Boosting
# ---------------------------
gb = Pipeline([
    ('prep', preprocess),
    ('model', GradientBoostingClassifier(random_state=42))
])

# Treinar
gb.fit(X_train, y_train)

# Predições
gb_preds_test = gb.predict(X_test)
gb_preds_train = gb.predict(X_train)

# Avaliar modelo
print(classification_report(y_test, gb_preds_test))

# Acurácias
gb_acc_test = accuracy_score(y_test, gb_preds_test)
gb_acc_train = accuracy_score(y_train, gb_preds_train)

print("Acurácia (TREINO) - Gradient Boosting:", round(gb_acc_train, 4))
print("Acurácia (TESTE)  - Gradient Boosting:", round(gb_acc_test, 4))

# Modelo 2: Naive Bayes
# ---------------------------
nb = Pipeline([
    ('prep', preprocess),
    ('model', GaussianNB())
])
nb.fit(X_train, y_train)
nb_preds = nb.predict(X_test)
print(classification_report(y_test, nb_preds))
nb_acc = accuracy_score(y_test, nb_preds)
print("Acurácia - Naive Bayes:", round(nb_acc, 4))

# Gradient Boosting Matriz confusão
# -----------------------
gb_cm = confusion_matrix(y_test, gb_preds_test)
disp = ConfusionMatrixDisplay(confusion_matrix=gb_cm, display_labels=gb.classes_)
plt.figure(figsize=(10, 8))
disp.plot(cmap="Blues", values_format="d")
plt.title("Matriz de Confusão - Gradient Boosting")
plt.show()

# pegar o modelo dentro do pipeline
gb_model = gb.named_steps['model']

# pegar nomes das features
feature_names = gb.named_steps['prep'].get_feature_names_out()

# pegar importâncias
importances = gb_model.feature_importances_

# ordenar
feat_imp = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values(by='importance', ascending=False)

feat_imp.head(10)

plt.figure(figsize=(10, 8))
plt.barh(feat_imp['feature'].head(10), feat_imp['importance'].head(10))
plt.gca().invert_yaxis()
plt.title("10 Features mais importantes – Gradient Boosting")
plt.show()